{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 卷积神经网络原理(CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "卷积神经网络CNN的结构一般包含这几个层：\n",
    "\n",
    "- 输入层：用于数据的输入\n",
    "- 卷积层：使用卷积核进行特征提取和特征映射\n",
    "- 激励层：由于卷积也是一种线性运算，因此需要增加非线性映射\n",
    "- 池化层：进行下采样，对特征图稀疏处理，减少数据运算量。\n",
    "- 全连接层：通常在CNN的尾部进行重新拟合，减少特征信息的损失"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN的三个特点：\n",
    "\n",
    "- 局部连接：这个是最容易想到的，每个神经元不再和上一层的所有神经元相连，而只和一小部分神经元相连。这样就减少了很多参数。\n",
    "- 权值共享：一组连接可以共享同一个权重，而不是每个连接有一个不同的权重，这样又减少了很多参数。\n",
    "- 下采样：可以使用Pooling来减少每层的样本数，进一步减少参数数量，同时还可以提升模型的鲁棒性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![cnn](cnn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 输入层\n",
    "\n",
    "在CNN的输入层中，（图片）数据输入的格式 与 全连接神经网络的输入格式（一维向量）不太一样。CNN的输入层的输入格式保留了图片本身的结构。\n",
    "\n",
    "对于黑白的 28×28 的图片，CNN的输入是一个 28×28 的的二维神经元：\n",
    "\n",
    "而对于RGB格式的28×28图片，CNN的输入则是一个 3×28×28 的三维神经元（RGB中的每一个颜色通道都有一个 28×28 的矩阵）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 卷积层\n",
    "\n",
    "这一层就是求内积"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![CNN](CNN.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "左边是输入，中间部分是两个不同的滤波器Filter w0、Filter w1，最右边则是两个不同的输出。\n",
    "\n",
    "最左边是输出为：\n",
    "\n",
    "$$ a_{i.j} = f(\\sum_{m=0}^{2}\\sum_{n=0}^{2}w_{m,n}x_{i+m,j+n} + w_b)$$\n",
    "\n",
    "以上图为例：\n",
    "\n",
    "- $w_{m,n}$:filter的第m行第n列的值\n",
    "- $x_{i,j}$: 表示图像的第i行第j列元素\n",
    "- $w_b$:用表示filter的偏置项\n",
    "- $a_{i,j}$:表示Feature Map的第i行第j列元素\n",
    "- $f$:表示Relu激活函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 激励层\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "激励层主要对卷积层的输出进行一个非线性映射，因为卷积层的计算还是一种线性计算。使用的激励函数一般为ReLu函数：\n",
    "\n",
    "$$f(x)=max(x,0)$$\n",
    "\n",
    "卷积层和激励层通常合并在一起称为“卷积层”。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 池化层\n",
    "\n",
    "当输入经过卷积层时，若感受视野比较小，布长stride比较小，得到的feature map （特征图）还是比较大，可以通过池化层来对每一个 feature map 进行降维操作，输出的深度还是不变的，依然为 feature map 的个数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "池化层也有一个“池化视野（filter）”来对feature map矩阵进行扫描，对“池化视野”中的矩阵值进行计算，一般有两种计算方式：\n",
    "\n",
    "- Max pooling：取“池化视野”矩阵中的最大值\n",
    "- Average pooling：取“池化视野”矩阵中的平均值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![maxpooling](maxpooling.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 全连接层\n",
    "\n",
    "- 和[神经网络](神经网络原理.ipynb)一样"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 卷积神经网络的训练\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. 前向计算每个神经元的输出值$a_j$（ 表示网络的第j个神经元，以下同）；\n",
    "2. 反向计算每个神经元的误差项$\\sigma_j， \\sigma_j$在有的文献中也叫做敏感度(sensitivity)。它实际上是网络的损失函数$E_d$对神经元加权输入的偏导数\n",
    "3. 计算每个神经元连接权重$w_{i,j}$的梯度（ $w_{i,j}$表示从神经元i连接到神经元j的权重）\n",
    "- 最后，根据梯度下降法则更新每个权重即可。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "** 实例 **\n",
    "\n",
    "[CNN实例](CNN经典实例.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
