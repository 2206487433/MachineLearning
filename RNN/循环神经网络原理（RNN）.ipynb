{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 循环神经网络原理（RNN）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- RNN是在自然语言处理领域中最先被用起来的\n",
    "- 语言模型就是这样的东西：给定一个一句话前面的部分，预测接下来最有可能的一个词是什么。\n",
    "- 简单的循环神经网络由输入层、一个隐藏层和一个输出层组成："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![RNN1.jpg](RNN1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- x是一个向量，它表示输入层的值（这里面没有画出来表示神经元节点的圆圈）；\n",
    "- s是一个向量，它表示隐藏层的值（这里隐藏层面画了一个节点，这一层其实是多个节点，节点数与向量s的维度相同）；\n",
    "- U是输入层到隐藏层的权重矩阵；\n",
    "- o也是一个向量，它表示输出层的值；\n",
    "- V是隐藏层到输出层的权重矩阵。\n",
    "- 循环神经网络的隐藏层的值s不仅仅取决于当前这次的输入x，还取决于上一次隐藏层的值s。\n",
    "- 权重矩阵 W就是隐藏层上一次的值作为这一次的输入的权重。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "展开后:\n",
    "\n",
    "![RNN2.jpg](RNN2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$o_t = g(Vs_t)$$    \n",
    "$$s_t = f(Ux_t + Ws_{t-1})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- f,g都是激活函数\n",
    "- 第一个式子是全连接层，第二个式子是循环层\n",
    "- 循环层和全连接层的区别就是循环层多了一个权重矩阵 W。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$o_t = g(Vs_t) \\\\\n",
    "= g(Vf(Ux_t + Ws_{t-1})) \\\\\n",
    "= g(Vf(Ux_t + Wf(Ux_{t-1} + Ws_{t-2})))\\\\\n",
    "= g(Vf(Ux_t + Wf(Ux_{t-1} + Wf(Ux_{t-2} + Ws_{t-3})))) \\\\   \n",
    "= g(Vf(Ux_t + Wf(Ux_{t-1} + Wf(Ux_{t-2} + Wf(Ux_{t-3} + ...)))))$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 循环神经网络的训练\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. 前向计算每个神经元的输出值$a_j$（ 表示网络的第j个神经元，以下同）；\n",
    "2. 反向计算每个神经元的误差项$\\sigma_j， \\sigma_j$在有的文献中也叫做敏感度(sensitivity)。它实际上是网络的损失函数$E_d$对神经元加权输入的偏导数\n",
    "3. 计算每个神经元连接权重$w_{i,j}$的梯度（ $w_{i,j}$表示从神经元i连接到神经元j的权重）\n",
    "- 最后，根据梯度下降法则更新每个权重即可。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 长短时记忆网络(LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "RNN有个缺点就是很难处理长距离依赖，就是记性差，LSTM成功解决了原始RNN的缺陷\n",
    "\n",
    "原始的RNN隐藏层只有一个状态，现在再增加一个状态c，让它保存长期的状态"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![LSTM](LSTM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "LSTM的输入有三个：\n",
    "\n",
    "- $x_t$:当前时刻网络的输入值\n",
    "- $h_{t-1}$:上一时刻LSTM的输出值\n",
    "- $c_{t-1}$:上一时刻的单元状态\n",
    "\n",
    "LSTM的输出有两个：\n",
    "\n",
    "- $h_t$:当前时刻LSTM输出值\n",
    "- $c_t$: 当前时刻的单元状态"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    ">LSTM的关键，就是怎样控制长期状态c。在这里，LSTM的思路是使用三个控制开关。第一个开关，负责控制继续保存长期状态c；第二个开关，负责控制把即时状态输入到长期状态c；第三个开关，负责控制是否把长期状态c作为当前的LSTM的输出。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "** 前向计算 **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "实现开关的算法，就是门，门实际上就是一层全连接层，它的输入是一个向量，输出是一个0到1之间的实数向量。假设W是门的权重向量，b是偏置项，那么门可以表示为：\n",
    "$$g(x) = \\sigma(Wx + b)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$\\sigma$是激活函数，值域为(0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "LSTM用两个门控制单元状态c的内容：\n",
    "- 遗忘门：决定了上一时刻的单元状态$c_{t-1}$有多少保留到当前时刻$c_t$\n",
    "- 输入门：决定了当前时刻网络的输入$x_t$有多少保存到单元状态$c_t$\n",
    "\n",
    "输出门控制单元状态$c_t$有多少输出到当先输出值$h_t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "遗忘门：\n",
    "\n",
    "$$f_t = \\sigma(W_f\\cdot[h_{t-1},x_t] + b_f)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![LSTM1](LSTM1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "输入门：\n",
    "\n",
    "$$i_t = \\sigma(W_i\\cdot[h_{t-1},x_t] + b_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![LSTM2](LSTM2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "当前输入状态$\\tilde{c_t}$:\n",
    "\n",
    "$$\\tilde{c_t} = tanh(W_c[h_{t-1},x_t] + b_c)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![LSTM3](LSTM3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "当前时刻的单元状态$c_t$:\n",
    "\n",
    "$$c_t = f_t\\circ c_{t-1} + i_t\\circ\\tilde{c_t}$$\n",
    "\n",
    "$\\circ$表示按元素乘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![LSTM4](LSTM4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "输出门：\n",
    "\n",
    "$$o_t = \\sigma(W_o\\cdot[h_{t-1},x_t] + b_o)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![LSTM5](LSTM5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "最终输出：\n",
    "\n",
    "$$h_t = o_t\\circ tanh(c_t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![LSTM6](LSTM6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "** 实例 **\n",
    "\n",
    "[RNN实例](RNN经典实例.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
