{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 垃圾邮件分类\n",
    "\n",
    "在DATA/email/spam文件夹中有25封垃圾邮件，在DATA/email/ham中有25封正常邮件，将其进行垃圾邮件分类。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "** 导入需要的库 **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "** 分词 **\n",
    "\n",
    "将邮件内容划分成一个个单词的形式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def textParse(bigString):\n",
    "    listOfTokens = re.split(r'\\W*', bigString)\n",
    "    return [tok.lower() for tok in listOfTokens if len(tok) > 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "> `re.split(r'\\W*', bigString)`, 表示除了数字、字母和下划线的符号进行划分，return是一个列表推到生成的列表，单词长度小于等于2的过率掉，并且将其变成小写字母"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['codeine',\n",
       " '15mg',\n",
       " 'for',\n",
       " '203',\n",
       " 'visa',\n",
       " 'only',\n",
       " 'codeine',\n",
       " 'methylmorphine',\n",
       " 'narcotic',\n",
       " 'opioid',\n",
       " 'pain',\n",
       " 'reliever',\n",
       " 'have',\n",
       " '15mg',\n",
       " '30mg',\n",
       " 'pills',\n",
       " '15mg',\n",
       " 'for',\n",
       " '203',\n",
       " '15mg',\n",
       " 'for',\n",
       " '385',\n",
       " '15mg',\n",
       " 'for',\n",
       " '562',\n",
       " 'visa',\n",
       " 'only']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textParse(open('../DATA/email/spam/1.txt').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['peter',\n",
       " 'with',\n",
       " 'jose',\n",
       " 'out',\n",
       " 'town',\n",
       " 'you',\n",
       " 'want',\n",
       " 'meet',\n",
       " 'once',\n",
       " 'while',\n",
       " 'keep',\n",
       " 'things',\n",
       " 'going',\n",
       " 'and',\n",
       " 'some',\n",
       " 'interesting',\n",
       " 'stuff',\n",
       " 'let',\n",
       " 'know',\n",
       " 'eugene']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textParse(open('../DATA/email/ham/1.txt').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "** 生成词汇表 **\n",
    "\n",
    "将所有的邮件进行分词后生成一个dataset，然后生成一个词汇表，这个词汇表是一个集合，每个单词出现一次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def createVocabList(dataSet):\n",
    "    vocabSet = set([])\n",
    "    for docment in dataSet:\n",
    "        vocabSet = vocabSet | set(docment)\n",
    "    return list(vocabSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['reliever',\n",
       " 'pills',\n",
       " '562',\n",
       " 'pain',\n",
       " 'only',\n",
       " '15mg',\n",
       " 'methylmorphine',\n",
       " 'for',\n",
       " 'codeine',\n",
       " '30mg',\n",
       " 'have',\n",
       " 'narcotic',\n",
       " 'visa',\n",
       " 'opioid',\n",
       " '385',\n",
       " '203']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_list=[]\n",
    "word_list = textParse(open('../DATA/email/spam/1.txt').read())\n",
    "doc_list.append(word_list)\n",
    "createVocabList(doc_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "** 生成词向量 **\n",
    "\n",
    "每一封邮件都存在了词汇表中，因此可以将每一封邮件生成一个词向量，存在几个则为几，不存在的为零，如上面`15mg`的词向量为`[0,1,0,...]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def bagOfWords2Vec(vocabList, inputSet):\n",
    "    returnVec = [0]*len(vocabList)\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] += 1\n",
    "        else:\n",
    "            print(\"the word is not in my vocabulry\")\n",
    "    return returnVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 2, 5, 1, 4, 2, 1, 1, 1, 2, 1, 1, 2]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bagOfWords2Vec(createVocabList(doc_list),doc_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "** 训练算法 **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "伪代码如下：\n",
    "\n",
    "```\n",
    "计算每个类别中的文档数目\n",
    "对每篇训练文档：\n",
    "    对每个类别：\n",
    "        如果词条出现在文档中——>增加该词条的计数值\n",
    "        增加所有词条的计数值\n",
    "对每个类别：\n",
    "    对每个词条：\n",
    "        将该词条的数目除以总词条数目得到条件概率\n",
    "返回每个类别的条件概率\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#这里的trainMat是训练样本的词向量，其是一个矩阵，他的每一行为一个邮件的词向量\n",
    "#trainGategory为与trainMat对应的类别，值为0，1表示正常，垃圾\n",
    "def train(trainMat, trainGategory):\n",
    "    numTrain = len(trainMat)\n",
    "    numWords = len(trainMat[0]) #is vocabulry length\n",
    "    pAbusive = sum(trainGategory)/float(numTrain)\n",
    "    p0Num = ones(numWords)\n",
    "    p1Num = ones(numWords)\n",
    "    p0Denom = 2.0\n",
    "    p1Denom = 2.0\n",
    "    for i in range(numTrain):\n",
    "        if trainGategory[i] == 1:\n",
    "            p1Num += trainMat[i]   #统计类1中每个单词的个数\n",
    "            p1Denom += sum(trainMat[i])  #类1的单词总数\n",
    "        else:\n",
    "            p0Num += trainMat[i]\n",
    "            p0Denom += sum(trainMat[i])\n",
    "    p1Vec = log(p1Num/p1Denom)  #类1中每个单词的概率\n",
    "    p0Vec =log(p0Num/p0Denom)\n",
    "    return p0Vec, p1Vec, pAbusive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "** 处理数据验证过程 **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#spam email classfy\n",
    "def spamTest():\n",
    "    fullTest=[];docList=[];classList=[]\n",
    "    for i in range(1,26): #it only 25 doc in every class\n",
    "        wordList=textParse(open('../DATA/email/spam/%d.txt' % i).read())\n",
    "        docList.append(wordList)\n",
    "        fullTest.extend(wordList)\n",
    "        classList.append(1)\n",
    "        wordList=textParse(open('../DATA/email/ham/%d.txt' % i).read())\n",
    "        docList.append(wordList)\n",
    "        fullTest.extend(wordList)\n",
    "        classList.append(0)\n",
    "    vocabList=createVocabList(docList)   # create vocabulry\n",
    "    trainSet=list(range(50));testSet=[]\n",
    "#choose 10 sample to test ,it index of trainMat\n",
    "    for i in range(10):\n",
    "        randIndex=int(random.uniform(0,len(trainSet)))#num in 0-49\n",
    "        testSet.append(trainSet[randIndex])\n",
    "        del(trainSet[randIndex])\n",
    "    trainMat=[];trainClass=[]\n",
    "    for docIndex in trainSet:\n",
    "        trainMat.append(bagOfWords2Vec(vocabList,docList[docIndex]))\n",
    "        trainClass.append(classList[docIndex])\n",
    "    p0,p1,pSpam=train(array(trainMat),array(trainClass))\n",
    "    errCount=0\n",
    "    for docIndex in testSet:\n",
    "        wordVec=bagOfWords2Vec(vocabList,docList[docIndex])\n",
    "        if classfy(array(wordVec),p0,p1,pSpam) != classList[docIndex]:\n",
    "            errCount +=1\n",
    "            print((\"classfication error\"), docList[docIndex])\n",
    "\n",
    "    print((\"the error rate is \") , float(errCount)/len(testSet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# classfy funtion\n",
    "def classfy(vec2classfy,p0Vec,p1Vec,pClass1):\n",
    "    p1=sum(vec2classfy*p1Vec)+log(pClass1)\n",
    "    p0=sum(vec2classfy*p0Vec)+log(1-pClass1)\n",
    "    if p1 > p0:\n",
    "        return 1;\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classfication error ['scifinance', 'now', 'automatically', 'generates', 'gpu', 'enabled', 'pricing', 'risk', 'model', 'source', 'code', 'that', 'runs', '300x', 'faster', 'than', 'serial', 'code', 'using', 'new', 'nvidia', 'fermi', 'class', 'tesla', 'series', 'gpu', 'scifinance', 'derivatives', 'pricing', 'and', 'risk', 'model', 'development', 'tool', 'that', 'automatically', 'generates', 'and', 'gpu', 'enabled', 'source', 'code', 'from', 'concise', 'high', 'level', 'model', 'specifications', 'parallel', 'computing', 'cuda', 'programming', 'expertise', 'required', 'scifinance', 'automatic', 'gpu', 'enabled', 'monte', 'carlo', 'pricing', 'model', 'source', 'code', 'generation', 'capabilities', 'have', 'been', 'significantly', 'extended', 'the', 'latest', 'release', 'this', 'includes']\n",
      "the error rate is  0.1\n",
      "classfication error ['scifinance', 'now', 'automatically', 'generates', 'gpu', 'enabled', 'pricing', 'risk', 'model', 'source', 'code', 'that', 'runs', '300x', 'faster', 'than', 'serial', 'code', 'using', 'new', 'nvidia', 'fermi', 'class', 'tesla', 'series', 'gpu', 'scifinance', 'derivatives', 'pricing', 'and', 'risk', 'model', 'development', 'tool', 'that', 'automatically', 'generates', 'and', 'gpu', 'enabled', 'source', 'code', 'from', 'concise', 'high', 'level', 'model', 'specifications', 'parallel', 'computing', 'cuda', 'programming', 'expertise', 'required', 'scifinance', 'automatic', 'gpu', 'enabled', 'monte', 'carlo', 'pricing', 'model', 'source', 'code', 'generation', 'capabilities', 'have', 'been', 'significantly', 'extended', 'the', 'latest', 'release', 'this', 'includes']\n",
      "the error rate is  0.1\n",
      "the error rate is  0.0\n",
      "the error rate is  0.0\n",
      "classfication error ['experience', 'with', 'biggerpenis', 'today', 'grow', 'inches', 'more', 'the', 'safest', 'most', 'effective', 'methods', 'of_penisen1argement', 'save', 'your', 'time', 'and', 'money', 'bettererections', 'with', 'effective', 'ma1eenhancement', 'products', 'ma1eenhancement', 'supplement', 'trusted', 'millions', 'buy', 'today']\n",
      "the error rate is  0.1\n",
      "classfication error ['benoit', 'mandelbrot', '1924', '2010', 'benoit', 'mandelbrot', '1924', '2010', 'wilmott', 'team', 'benoit', 'mandelbrot', 'the', 'mathematician', 'the', 'father', 'fractal', 'mathematics', 'and', 'advocate', 'more', 'sophisticated', 'modelling', 'quantitative', 'finance', 'died', '14th', 'october', '2010', 'aged', 'wilmott', 'magazine', 'has', 'often', 'featured', 'mandelbrot', 'his', 'ideas', 'and', 'the', 'work', 'others', 'inspired', 'his', 'fundamental', 'insights', 'you', 'must', 'logged', 'view', 'these', 'articles', 'from', 'past', 'issues', 'wilmott', 'magazine']\n",
      "classfication error ['yay', 'you', 'both', 'doing', 'fine', 'working', 'mba', 'design', 'strategy', 'cca', 'top', 'art', 'school', 'new', 'program', 'focusing', 'more', 'right', 'brained', 'creative', 'and', 'strategic', 'approach', 'management', 'the', 'way', 'done', 'today']\n",
      "the error rate is  0.2\n",
      "the error rate is  0.0\n",
      "classfication error ['scifinance', 'now', 'automatically', 'generates', 'gpu', 'enabled', 'pricing', 'risk', 'model', 'source', 'code', 'that', 'runs', '300x', 'faster', 'than', 'serial', 'code', 'using', 'new', 'nvidia', 'fermi', 'class', 'tesla', 'series', 'gpu', 'scifinance', 'derivatives', 'pricing', 'and', 'risk', 'model', 'development', 'tool', 'that', 'automatically', 'generates', 'and', 'gpu', 'enabled', 'source', 'code', 'from', 'concise', 'high', 'level', 'model', 'specifications', 'parallel', 'computing', 'cuda', 'programming', 'expertise', 'required', 'scifinance', 'automatic', 'gpu', 'enabled', 'monte', 'carlo', 'pricing', 'model', 'source', 'code', 'generation', 'capabilities', 'have', 'been', 'significantly', 'extended', 'the', 'latest', 'release', 'this', 'includes']\n",
      "the error rate is  0.1\n",
      "the error rate is  0.0\n",
      "classfication error ['yeah', 'ready', 'may', 'not', 'here', 'because', 'jar', 'jar', 'has', 'plane', 'tickets', 'germany', 'for']\n",
      "classfication error ['home', 'based', 'business', 'opportunity', 'knocking', 'your', 'door', 'don抰', 'rude', 'and', 'let', 'this', 'chance', 'you', 'can', 'earn', 'great', 'income', 'and', 'find', 'your', 'financial', 'life', 'transformed', 'learn', 'more', 'here', 'your', 'success', 'work', 'from', 'home', 'finder', 'experts']\n",
      "the error rate is  0.2\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    spamTest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
