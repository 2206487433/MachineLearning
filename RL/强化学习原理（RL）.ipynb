{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 强化学习原理（RL）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## RL与有监督学习、无监督学习的比较： \n",
    "　"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- 有监督的学习是从一个已经标记的训练集中进行学习，训练集中每一个样本的特征可以视为是对该situation的描述，而其label可以视为是应该执行的正确的action，但是有监督的学习不能学习交互的情景，因为在交互的问题中获得期望行为的样例是非常不实际的，agent只能从自己的经历（experience）中进行学习，而experience中采取的行为并一定是最优的。这时利用RL就非常合适，因为RL不是利用正确的行为来指导，而是利用已有的训练信息来对行为进行评价。 \n",
    "\n",
    "\n",
    "- 因为RL利用的并不是采取正确行动的experience，从这一点来看和无监督的学习确实有点像，但是还是不一样的，无监督的学习的目的可以说是从一堆未标记样本中发现隐藏的结构，而RL的目的是最大化reward signal。 \n",
    "\n",
    "\n",
    "- 总的来说，RL与其他机器学习算法不同的地方在于：其中没有监督者，只有一个reward信号；反馈是延迟的，不是立即生成的；时间在RL中具有重要的意义；agent的行为会影响之后一系列的data。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- 没有教师信号，也没有label。只有reward，其实reward就相当于label。\n",
    "- 反馈有延时，不是能立即返回。\n",
    "- 相当于输入数据是序列数据。\n",
    "- agent执行的动作会影响之后的数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 强化学习的关键要素"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- environment\n",
    "- reward\n",
    "- action\n",
    "- state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "从要完成的任务提取一个环境，从中抽象出状态(state) 、动作(action)、以及执行该动作所接受的瞬时奖赏(reward)。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "** Reward **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "reward通常都被记作$R_t$，表示第t个time step的返回奖赏值。所有强化学习都是基于reward假设的。reward是一个scalar。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "** Action **\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "action是来自于动作空间，agent对每次所处的state用以及上一状态的reward确定当前要执行什么action。执行action要达到最大化期望reward，直到最终算法收敛，所得的policy就是一系列action的sequential data。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "** state **\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "就是指当前agent所处的状态。具体来讲，例如玩pong游戏，该游戏的状态就是当前time step下小球的位置。而Flappy bird状态就是当前小鸟在平面上的位置。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "** policy **\n",
    "\n",
    "policy就是只agent的行为，是从state到action的映射，分为确定策略和与随机策略，确定策略就是某一状态下的确定动作$a=π(s)$, 随机策略以概率来描述，即某一状态下执行这一动作的概率：$π(a|s)=P[A_t=a|S_t=s]$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "** value function **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "因为强化学习今本上可以总结为通过最大化reward来得到一个最优策略。但是如果只是瞬时reward最大会导致每次都只会从动作空间选择reward最大的那个动作，这显然不是我们想要的，所以为了很好地刻画是包括未来的当前reward值最大（即使从当前时刻开始一直到状态达到目标的总reward最大）。因此就构造了值函数（value function）来描述这一变量。表达式如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$ V_{\\pi}(s) = E[R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + ...| S_t = s]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$γ$是一个取值在[0,1]的数，就是为了减少未来的reward对当前动作的影响。然后就通过选取合适的policy使value function最大"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "** model **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "model就是用来预测环境接下来会干什么，即在这一状态的情况下执行某一动作会达到什么样的状态，这一个动作会得到什么reward。所以描述一个模型就是用动作转移概率与动作状态reward。具体公式如下： "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$ p_{ss^{'}}^a = P[S_{t+1} = s^{'} | S_t = s, A_t = a]$\n",
    "\n",
    "$ R_s^a = E[R_{t+1} | S_t = s, A_t = a]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[增强学习](http://dataunion.org/28875.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "** 实例 **\n",
    "\n",
    "[强化学习应用实例](强化学习应用实例.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
