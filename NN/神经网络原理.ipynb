{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 神经网络原理\n",
    "\n",
    "- 感知机学习算法\n",
    "- 神经网络\n",
    "    - 从感知机到神经网络\n",
    "    - 多层前馈神经网络\n",
    "    - bp算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 感知机学习算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- 感知机（perceptron）是二分类的线性分类模型，属于监督学习算法。输入为实例的特征向量，输出为实例的类别（取+1和-1）。感知机对应于输入空间中将实例划分为两类的分离超平面。感知机旨在求出该超平面，为求得超平面导入了基于误分类的损失函数，利用梯度下降法 对损失函数进行最优化（最优化）。\n",
    "- 是神经网络和支持向量机的基础。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "** 感知机定义 **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "假设输入空间(特征向量)为$X\\subseteq Rn$，输出空间为$Y={-1, +1}$。输入$x\\in X$表示实例的特征向量，对应于输入空间的点；输出$y\\in Y$表示示例的类别。由输入空间到输出空间的函数为"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$f(\\vec{x}) = sign(\\vec{w}\\cdot\\vec{x} + b)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$sign(x)=\n",
    "\\begin{cases}\n",
    "+1& \\text{$if x \\ge 0$}\\\\\n",
    "-1& \\text{else}\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "** 感知机学习策略 **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "如果训练集是可分的，感知机的学习目的是求得一个能将训练集正实例点和负实例点完全分开的分离超平面。为了找到这样一个平面（或超平面），即确定感知机模型参数$\\vec{w}$和b，我们采用的是损失函数，同时并将损失函数极小化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- 对于正确分类的样本点$(\\vec{x_i},y_i)$, 有$(\\vec{w}\\cdot\\vec{x_i} + b)y_i > 0$\n",
    "- 对于误分类的样本点$(\\vec{x_i},y_i)$, 有$(\\vec{w}\\cdot\\vec{x_i} + b)y_i < 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "误分类点到超平面的距离:\n",
    "\n",
    "$$\\frac{1}{||\\vec{w}||_2}|(\\vec{w}\\cdot\\vec{x_i} + b)|$$\n",
    "\n",
    "$||\\vec{w}||_2$为$\\vec{w}$的$L_2$范数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "对于误分类点：\n",
    "\n",
    "$$-(\\vec{w}\\cdot\\vec{x_i} + b)y_i > 0$$    \n",
    "$$-\\frac{1}{||\\vec{w}||_2}(\\vec{w}\\cdot\\vec{x_i} + b)y_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "所有的点到超平面的距离:\n",
    "\n",
    "$$-\\frac{1}{||\\vec{w}||_2}\\sum_{\\vec{x_i} \\in M}(\\vec{w}\\cdot\\vec{x_i} + b)y_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "不考虑$\\frac{1}{||\\vec{w}||_2}$，就得到了感知机的损失函数：\n",
    "\n",
    "$$ L(\\vec{w},b) =-\\sum_{\\vec{x_i} \\in M}(\\vec{w}\\cdot\\vec{x_i} + b)y_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "** 感知机学习算法 **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "感知机学习转变成求解损失函数$L(\\vec{w},b)$的最优化问题。最优化的方法是随机梯度下降法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\min_{\\vec{w},b}L(\\vec{w},b) = -\\min_{\\vec{w},b} \\sum_{\\vec{x_i} \\in M}(\\vec{w}\\cdot\\vec{x_i} + b)y_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "定义损失函数的梯度：\n",
    "\n",
    "$$\\nabla_{\\vec{w}}L(\\vec{w},b) = - \\sum_{\\vec{x_i} \\in M}y_i\\vec{x_i}$$   \n",
    "$$\\nabla_{b}L(\\vec{w},b) = - \\sum_{\\vec{x_i} \\in M}y_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "随机选取一个误分类点，更新$\\vec{w},b$的值：\n",
    "\n",
    "$$\\vec{w} := \\vec{w} + \\eta y_i \\vec{x_i}$$  \n",
    "$$b := b + \\eta y_i$$\n",
    "\n",
    "> $\\eta \\in (0,1]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### 从感知机到神经网络\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- 感知机可以看作神经网络的特例。感知机由两层神经元组成：输入层接收外界输入信号，输出层是M-P神经元。\n",
    "- 感知机只有输出层神经元进行激活函数处理，即只拥有一层功能神经元"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 多层前馈神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- 感知机只拥有一层功能神经元，它只能处理线性可分的问题，要想解决非线性可分问题，可以使用多层功能神经元\n",
    "- 神经网络的结构：\n",
    "    - 每层神经元与下一层神经元全部相连\n",
    "    - 同层神经元之间不存在连接\n",
    "    - 跨层神经元之间也不存在连接"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- 多层前馈神经网络有一下特点：\n",
    "    - 掩藏层和输出层神经元都拥有激活函数\n",
    "    - 输入层接收外界输入信号，不进行激活函数处理\n",
    "    - 最终结果由输出层神经元给出\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "下图是一个简单的神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![FP](FP.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "** 激活函数 **\n",
    "\n",
    "隐藏层和输出层都需要激活函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "我们选用sigmod函数作为激活函数：\n",
    "\n",
    "$$z_i^{(l)} = \\sum_{j=1}^{n} W_{ij}^{(l-1)}a_j^{(l-1)} + b_i^{(l-1)}$$\n",
    "$$f(z_i^{(l)}) = \\frac{1}{1+e^{-z_i^{(l)}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "参数说明：\n",
    "\n",
    "- $a_i^{(l)}:表示第l层第i个单元的输出值$\n",
    "- $W_{ij}^{(l)}:表示第l层第j单元和第l+1层第i单元之间的权重$\n",
    "- $b_i^{(l)}: 表示第l+1层第i单元的偏置项$\n",
    "- $z_i^{(l)}: 表示第l层第i个单元输入加权和$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "对于给定参数集合 $\\textstyle W,b $，我们的神经网络就可以按照函数 $\\textstyle h_{W,b}(x) $来计算输出结果。上图神经网络的计算步骤如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$a_1^{(2)} = f(W_{11}^{(1)}x_1 + W_{12}^{(1)}x_2 + W_{13}^{(1)}x_3 + b_1^{(1)})$$\n",
    "$$a_2^{(2)} = f(W_{21}^{(1)}x_1 + W_{22}^{(1)}x_2 + W_{23}^{(1)}x_3 + b_1^{(1)})$$\n",
    "$$a_3^{(2)} = f(W_{31}^{(1)}x_1 + W_{32}^{(1)}x_2 + W_{33}^{(1)}x_3 + b_2^{(1)})$$\n",
    "$$\\textstyle h_{W,b}(x)=a_1^{(3)} = f(W_{11}^{(1)}x_1 + W_{12}^{(1)}x_2 + W_{13}^{(1)}x_3 + b_1^{(1)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "上面的计算步骤叫做前向传播"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 反向传播算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "** 代价函数 **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ C = \\frac{1}{2n}\\sum_{i=1}^{n}||y_i(x)-a_i^{(L)}(x)||^2$$\n",
    "\n",
    "> $\\textstyle 其中，x表示输入的样本，y表示实际的分类，a^{(L)}表示预测的输出，L表示神经网络的最大层数。$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "** 公式及其推导 **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "首先，将第l层第i个神经元中产生的错误（即实际值与预测值之间的误差）定义为：\n",
    "\n",
    "$$\\delta_i^{(l)} \\equiv \\frac{\\partial C}{\\partial z_i^{(l)}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "为了便于理解，下面都是一个样本"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "最后一层神经网络产生的错误：\n",
    "\n",
    "$$  \\delta_i^{(L)} = \\nabla_aC\\odot f^{'}(z_i^{(L)})$$\n",
    "\n",
    "> $\\odot$ 用于矩阵或向量之间点对点的乘法运算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "由后往前，计算每一层神经网络产生的错误：\n",
    "\n",
    "$$  \\delta_i^{(l)} = ((w^{(l)})^T \\delta_i^{(l+1)})\\odot f^{'}(z_i^{(l)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "权重的梯度：\n",
    "\n",
    "$$ \\frac{\\partial C}{\\partial w_{ij}^{(l)}} = a_j^{(l)}\\delta_i^{(l+1)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "偏置的梯度：\n",
    "\n",
    "$$ \\frac{\\partial C}{\\partial b_i^{(l)}} = \\delta_i^{(l)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "使用梯度下降，训练参数：\n",
    "\n",
    "$$w^{(l)} := w^{(l)} - \\frac{\\eta}{m}\\sum_{i=1}{n}\\delta_i^{(l)}(a_i^{(l)})^T$$  \n",
    "$$b^{(l)} := b^{(l)} - \\frac{\\eta}{m}\\sum_{i=1}{n}\\delta_i^{(l)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "** 实例 **\n",
    "\n",
    "[神经网络应用实例](神经网络应用实例.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
